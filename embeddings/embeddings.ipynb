{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvqax6oEQxHm"
      },
      "source": [
        "# Embeddings con OpenAI\n",
        "Embeddings es un proceso mediante el cual se utiliza alguna tecnica/algoritmo que sea capaz de convertir palabras o texto a vectores de N dimensiones. Estos vectores contienen cierto nivel de información semantica sobre el texto o palabra. Por ejemplo, palabras que son muy similares van a tener valores cercanos en sus representaciones en vectores.\n",
        "\n",
        "Hay varios modelos que son capaces de hacer un embedding de nuestro texto, en este cuaderno estaremos utilizando el embedding de OpenAI, el cual tiene la capacidad de posicionas muy bien palabras o textos segun su semantica. Este es el mismo embedding que utiliza GPT3. En este cuaderno estarás haciendo embedding de un texto para despues poder buscar cosas dentro de este texto por medio de preguntas en lenguaje natural."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VsDTcQdWQxHo",
        "outputId": "7c4199c0-c419-4100-e1df-486535ad4310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.47.1-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.0 (from gradio)\n",
            "  Downloading gradio_client-0.6.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.13)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=2bddf780fe9f5ab8f310c159858f48ecfbaf02863a66e7689e77ae2fad65b363\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, semantic-version, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.103.2 ffmpy-0.3.1 gradio-3.47.1 gradio-client-0.6.0 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.18.0 orjson-3.9.9 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uvicorn-0.23.2 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install gradio\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "from openai.embeddings_utils import get_embedding\n",
        "from openai.embeddings_utils import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XLjUTg1GQxHq"
      },
      "outputs": [],
      "source": [
        "# Definimos la API Key para vincular el cuaderno con nuestra cuenta de OpenAI\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zC01pouQxHr"
      },
      "source": [
        "# Que es y cómo usar embeddings\n",
        "Al hacer embedding de un dato, lo estamos convirtiendo a un vector numérico, datos similares estarán más cercanos entre si cuando semanticamente son similares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eGQobTZAQxHr"
      },
      "outputs": [],
      "source": [
        "# Se puede hacer embeeding de palabras o cadenas de texto\n",
        "palabras = [\"casa\", \"perro\", \"gato\", \"lobo\", \"leon\", \"zebra\", \"tigre\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ge7NhwjQxHr",
        "outputId": "9ddaf678-a587-4f68-fdef-ff34b6649d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RetryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/embeddings_utils.py\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(text, engine, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/embedding.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    709\u001b[0m             return (\n\u001b[0;32m--> 710\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    711\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    776\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for text-embedding-ada-002 in organization org-tS3AjMejcHFcyKDmUihPen9n on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-eae86ff5d820>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdiccionario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpalabras\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdiccionario\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-embedding-ada-002\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x78fbcda66860 state=finished raised RateLimitError>]"
          ]
        }
      ],
      "source": [
        "diccionario = {}\n",
        "for i in palabras:\n",
        "    diccionario[i] = get_embedding(i, engine=\"text-embedding-ada-002\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ28KBUWQxHr",
        "outputId": "956e1876-2ed3-440a-e6e2-c0a117cd42b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['casa', 'perro', 'gato', 'lobo', 'leon', 'zebra', 'tigre'])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diccionario.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfqmEAI_QxHs"
      },
      "outputs": [],
      "source": [
        "palabra = \"gato\"\n",
        "print(\"Primeros 10 valores de {}:\\n\".format(palabra), diccionario[palabra][:10])\n",
        "print(\"\\n\")\n",
        "print(\"Número de dimensiones del dato embebido\\n\", len(diccionario[palabra]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0jhFst3QxHs"
      },
      "source": [
        "## Comparar dos embeddings\n",
        "Debido a que los embeddings son una representacion vectorial de los datos en un espacio latente, podemos medir la distancia entre dos vectores y asi obtener que tan similares son. Podemos comparar una palabra nueva o alguna de las que ya fueron embebidas\n",
        "OJO: No necesariamente es similitud al objeto. Ej. perro y gato aun siendo \"opuestos\" semanticamente estan cerca pues tienen una relación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MaHi-2eQxHt",
        "outputId": "f8afca0f-601a-46f5-90e4-98071b722baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8159836735319491\n"
          ]
        }
      ],
      "source": [
        "n_palabra = \"agujero negro\" # Palabra nueva a comparar\n",
        "palabra_comparar = \"perro\" # Palabra del diccionario con la que compararemos la nueva palabra\n",
        "n_palabra_embed = get_embedding(n_palabra, engine=\"text-embedding-ada-002\")\n",
        "similitud = cosine_similarity(diccionario[palabra_comparar], n_palabra_embed)\n",
        "print(similitud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1eTXVTDQxHt"
      },
      "source": [
        "# Sumar embeddings\n",
        "Como los vectores contienen valores numericos, podemos sumarlos y el resultado será un nuevo vector de un concepto que una los elementos sumados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q9xOYjvQxHt",
        "outputId": "0d2885c5-d184-485d-ce6b-2e7a96be9665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "casa : [0.82819415]\n",
            "perro : [0.85046401]\n",
            "gato : [0.85630059]\n",
            "lobo : [0.85639036]\n",
            "leon : [0.9531031]\n",
            "zebra : [0.95310309]\n",
            "tigre : [0.88060081]\n"
          ]
        }
      ],
      "source": [
        "# Suma dos listas usando pandas\n",
        "sumados = (pd.DataFrame(diccionario[\"leon\"])) + (pd.DataFrame(diccionario[\"zebra\"]))\n",
        "len(sumados)\n",
        "\n",
        "for key, value in diccionario.items():\n",
        "    print(key, \":\", cosine_similarity(diccionario[key], sumados))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srxh8ACjQxHu"
      },
      "source": [
        "# Aplicacion de un Chatbot\n",
        "\n",
        "Usaremos Gradio para hacer una interfaz básica donde podremos hacer preguntas y obtendremos una respuesta.\n",
        "Para esto reutilizaremos lo que hemos visto hasta el momento pero usaremos el archivo de **chatbot_qa.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlRoM-NoQxHu",
        "outputId": "6138a715-aede-4109-ec45-c35ba4632bbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def embed_text(path=\"texto.csv\"):\n",
        "    conocimiento_df = pd.read_csv(path)\n",
        "    conocimiento_df['Embedding'] = conocimiento_df['texto'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
        "    conocimiento_df.to_csv('embeddings.csv')\n",
        "    return conocimiento_df\n",
        "\n",
        "def buscar(busqueda, datos, n_resultados=5):\n",
        "    busqueda_embed = get_embedding(busqueda, engine=\"text-embedding-ada-002\")\n",
        "    datos[\"Similitud\"] = datos['Embedding'].apply(lambda x: cosine_similarity(x, busqueda_embed))\n",
        "    datos = datos.sort_values(\"Similitud\", ascending=False)\n",
        "    return datos.iloc[:n_resultados][[\"texto\", \"Similitud\", \"Embedding\"]]\n",
        "\n",
        "texto_emb = embed_text(\"./chatbot_qa.csv\")\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    busqueda = gr.Textbox(label=\"Buscar\")\n",
        "    output = gr.DataFrame(headers=['texto'])\n",
        "    greet_btn = gr.Button(\"Preguntar\")\n",
        "    greet_btn.click(fn=buscar, inputs=[busqueda, gr.DataFrame(texto_emb)], outputs=output)\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi1B6wUwQxHu"
      },
      "source": [
        "# Procesar datos de un PDF\n",
        "Haremos ahora un ejemplo donde leemos un PDF para poder hacer preguntas y traer un exctracto del PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lG_cyNzPQxHv",
        "outputId": "339db87c-34b9-4dce-9124-89b569e77871",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.314)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.16.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.43)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain pypdf\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"/constitucion_bolivia.pdf\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jkNcIgjbUz01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4fa_yh1iQxHv",
        "outputId": "b8ecf209-6d1e-4ae1-da79-5889d165a713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Constitución Política del Estado (CPE) - Bolivia - InfoLeyes - Legislación online\\nhttp://bolivia.infoleyes.com/shownorm.php?id=469 [12/28/2011 4:30:59 PM]CONSTITUCION POLITICA DEL ESTADO\\nPRIMERA PARTE\\nBASES FUNDAMENTALES DEL ESTADO\\nDERECHOS, DEBERES Y GARANTÍAS\\nTÍTULO I\\nBASES FUNDAMENTALES DEL ESTADO\\nCAPÍTULO PRIMERO\\nMODELO DE ESTADO\\nArtículo 1.\\nBolivia se constituye en un Estado Unitario Social de Derecho Plurinacional Comunitario, libre, independiente,\\nsoberano, democrático, intercultural, descentralizado y con autonomías. Bolivia se funda en la pluralidad y el\\npluralismo político, económico, jurídico, cultural y lingüístico, dentro del proceso integrador del país.\\nArtículo 2.\\nDada la existencia precolonial de las naciones y pueblos indígena originario campesinos y su dominio ancestral\\nsobre sus territorios, se garantiza su libre determinación en el marco de la unidad del Estado, que consiste en\\nsu derecho a la autonomía, al autogobierno, a su cultura, al reconocimiento de sus instituciones y a la\\nconsolidación de sus entidades territoriales, conforme a esta Constitución y la ley.\\nArtículo 3.\\nLa nación boliviana está conformada por la totalidad de las bolivianas y los bolivianos, las naciones y pueblos\\nindígena originario campesinos, y las comunidades interculturales y afrobolivianas que en conjunto constituyen\\nel pueblo boliviano.\\nArtículo 4.\\nEl Estado respeta y garantiza la libertad de religión y de creencias espirituales, de acuerdo con sus\\ncosmovisiones. El Estado es independiente de la religión.\\nArtículo 5.\\nI. Son idiomas oficiales del Estado el castellano y todos los idiomas de las naciones y pueblos indígena\\noriginario campesinos, que son el aymara, araona, baure, bésiro, canichana, cavineño, cayubaba,\\nchácobo, chimán, ese ejja, guaraní, guarasu’we, guarayu, itonama, leco, machajuyai-kallawaya,\\nmachineri, maropa, mojeño-trinitario, mojeño-ignaciano, moré, mosetén, movima, pacawara, puquina,\\nquechua, sirionó, tacana, tapiete, toromona, uru-chipaya, weenhayek, yaminawa, yuki, yuracaré y\\nzamuco.\\nII. El Gobierno plurinacional y los gobiernos departamentales deben utilizar al menos dos idiomas oficiales.Uno de ellos debe ser el castellano, y el otro se decidirá tomando en cuenta el uso, la conveniencia, las\\ncircunstancias, las necesidades y preferencias de la población en su totalidad o del territorio en cuestión.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Un elemento por cada página\n",
        "pages[1].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NUV_6tzGQxHv"
      },
      "outputs": [],
      "source": [
        "# Objeto que va a hacer los cortes en el texto\n",
        "split = CharacterTextSplitter(chunk_size=300, separator = '.\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8RHj8aK3QxHv",
        "outputId": "e6a771cb-7ff6-4fd3-c4eb-5c5a2b161444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 901, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 412, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 447, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 396, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 346, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 326, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 410, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 495, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 307, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 391, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 339, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 471, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 322, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 387, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 522, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 307, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 314, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 315, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 726, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 712, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 306, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 340, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 328, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 328, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 428, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 323, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 301, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 373, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 401, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 344, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 498, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 310, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 394, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 412, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 360, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 406, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 387, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 336, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 515, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 311, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 327, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 322, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 318, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 330, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 457, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 461, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 556, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 577, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 376, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 317, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 348, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 327, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 440, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 330, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 352, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 302, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 342, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 382, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 509, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 587, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 452, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 456, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 385, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 611, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 338, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 341, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 339, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 332, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 503, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 587, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 335, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 336, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 452, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 389, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 397, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 484, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 502, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 358, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 407, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 371, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 452, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 329, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 502, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 367, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 334, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 437, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 417, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 303, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 476, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 350, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 334, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 425, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 311, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 339, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 397, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 573, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 329, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 409, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 313, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 334, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 638, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 449, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 511, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 305, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 400, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 345, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 371, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 386, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 483, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 589, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 522, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 388, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 434, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 402, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 440, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 406, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 394, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 306, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 354, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 314, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 356, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 304, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 383, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 422, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 360, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 443, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 310, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 314, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 368, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 463, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 347, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 414, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 328, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 386, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 556, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 522, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 471, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 330, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 365, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 552, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 316, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 346, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 379, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 309, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 319, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 420, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 447, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 356, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 347, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 302, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 310, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 307, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 442, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 325, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 347, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 316, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 448, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 334, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 311, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 323, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 410, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 498, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 404, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 473, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 304, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 373, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 479, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 414, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 433, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 357, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 421, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 357, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 362, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 314, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 318, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 357, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 484, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 391, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 361, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 459, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 335, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 422, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 475, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 423, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 383, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 358, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 428, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 339, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 440, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 305, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 307, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 388, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 344, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 518, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 482, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 317, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 357, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 394, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 304, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 406, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 460, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 359, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 359, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 435, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 543, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 595, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 413, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 392, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 377, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 504, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 338, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 408, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 323, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 617, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 480, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 374, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 311, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 500, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 790, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 403, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 318, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 442, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 357, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 379, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 332, which is longer than the specified 300\n"
          ]
        }
      ],
      "source": [
        "textos = split.split_documents(pages) # Lista de textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NHcme_rZQxHv",
        "outputId": "4fb74b0c-100a-417a-f561-0be462953771",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artículo 16.\n",
            "I. Toda persona tiene derecho al agua y a la alimentación.\n",
            "II. El Estado tiene la obligación de garantizar la seguridad alimentaria, a través de una alimentación sana,\n",
            "adecuada y suficiente para toda la población.\n",
            "Artículo 17\n"
          ]
        }
      ],
      "source": [
        "print(textos[50])\n",
        "#print(textos[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "T7gkHbIMQxHw",
        "outputId": "3cdab510-ae3b-4bef-e85f-2e04fb35e3a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-4b15ba61b8ed>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extraemos la parte de page_content de cada texto y lo pasamos a un dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtextos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Lista de parrafos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mparrafos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"texto\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparrafos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-4b15ba61b8ed>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extraemos la parte de page_content de cada texto y lo pasamos a un dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtextos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Lista de parrafos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mparrafos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"texto\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparrafos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
          ]
        }
      ],
      "source": [
        "# Extraemos la parte de page_content de cada texto y lo pasamos a un dataframe\n",
        "textos = [str(i.page_content) for i in textos] #Lista de parrafos\n",
        "parrafos = pd.DataFrame(textos, columns=[\"texto\"])\n",
        "print(parrafos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dkxMeLsPQxHw",
        "outputId": "64d28594-1bb2-4f50-f72b-8c27faa6b816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RetryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/embeddings_utils.py\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(text, engine, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/embedding.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    709\u001b[0m             return (\n\u001b[0;32m--> 710\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    711\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    776\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for text-embedding-ada-002 in organization org-tS3AjMejcHFcyKDmUihPen9n on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-dcf47c6f2ac9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparrafos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Embedding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparrafos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"texto\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text-embedding-ada-002'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Nueva columna con los embeddings de los parrafos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparrafos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MTG.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-dcf47c6f2ac9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparrafos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Embedding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparrafos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"texto\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text-embedding-ada-002'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Nueva columna con los embeddings de los parrafos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparrafos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MTG.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x78fbc6743ee0 state=finished raised RateLimitError>]"
          ]
        }
      ],
      "source": [
        "parrafos['Embedding'] = parrafos[\"texto\"].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002')) # Nueva columna con los embeddings de los parrafos\n",
        "parrafos.to_csv('MTG.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg3u3MisQxHw",
        "outputId": "0d954900-c05d-415e-d825-39f3a0d4feb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7863\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# La misma funcion del chatbot de pregunts y respuestas\n",
        "def embed_text(path=\"texto.csv\"):\n",
        "    conocimiento_df = pd.read_csv(path)\n",
        "    conocimiento_df['Embedding'] = conocimiento_df['texto'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
        "    conocimiento_df.to_csv('mtg-embeddings.csv')\n",
        "    return conocimiento_df\n",
        "\n",
        "def buscar(busqueda, datos, n_resultados=5):\n",
        "    busqueda_embed = get_embedding(busqueda, engine=\"text-embedding-ada-002\")\n",
        "    datos[\"Similitud\"] = datos['Embedding'].apply(lambda x: cosine_similarity(x, busqueda_embed))\n",
        "    datos = datos.sort_values(\"Similitud\", ascending=False)\n",
        "    return datos.iloc[:n_resultados][[\"texto\", \"Similitud\", \"Embedding\"]]\n",
        "\n",
        "texto_emb = parrafos\n",
        "with gr.Blocks() as demo:\n",
        "    busqueda = gr.Textbox(label=\"Buscar\")\n",
        "    output = gr.DataFrame(headers=['texto'])\n",
        "    greet_btn = gr.Button(\"Preguntar\")\n",
        "    greet_btn.click(fn=buscar, inputs=[busqueda, gr.DataFrame(texto_emb)], outputs=output)\n",
        "\n",
        "demo.launch()\n",
        "\n",
        "\n",
        "# resp = buscar(\"Con cuanta vida empiezo?\", parrafos, 5) # Reutilizamos la funcion de \"buscar\" del app de gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BhPTflZQxHw"
      },
      "outputs": [],
      "source": [
        "# resp = buscar(\"Con cuanta vida empiezo?\", parrafos, 5) # Reutilizamos la funcion de \"buscar\" del app de gradio\n",
        "# print(resp.texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ltB-gV0QxHx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.16 ('gpt3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e3e99415c7ce2cb9b8857283077fa90c1d7ffec737ddc9f365b1225d7f13a404"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}